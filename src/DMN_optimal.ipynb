{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqNQIqzJo8al"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3LUEFds8FAQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n",
    "\n",
    "def flatten(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "def generate_batches(batch_size, training_data):\n",
    "    random.shuffle(training_data)\n",
    "    start_index = 0\n",
    "    end_index = batch_size\n",
    "    while end_index < len(training_data):\n",
    "        batch = training_data[start_index:end_index]\n",
    "        temp = end_index\n",
    "        end_index += batch_size\n",
    "        start_index = temp\n",
    "        yield batch\n",
    "\n",
    "    if end_index >= len(training_data):\n",
    "        batch = training_data[start_index:]\n",
    "        yield batch\n",
    "\n",
    "def pad_batch(batch, word_to_index):\n",
    "    facts, questions, answers = zip(*batch)\n",
    "    max_fact_count = max(len(f) for f in facts)\n",
    "    max_fact_length = max(f.size(1) for f in flatten(facts))\n",
    "    max_question_length = max(q.size(1) for q in questions)\n",
    "    max_answer_length = max(a.size(1) for a in answers)\n",
    "\n",
    "    padded_facts, fact_masks, padded_questions, padded_answers = [], [], [], []\n",
    "    for i in range(len(batch)):\n",
    "        padded_fact = []\n",
    "        for fact in facts[i]:\n",
    "            if fact.size(1) < max_fact_length:\n",
    "                padded_fact.append(torch.cat([fact, Variable(LongTensor([word_to_index['<PAD>']] * (max_fact_length - fact.size(1)))).view(1, -1)], 1))\n",
    "            else:\n",
    "                padded_fact.append(fact)\n",
    "\n",
    "        while len(padded_fact) < max_fact_count:\n",
    "            padded_fact.append(Variable(LongTensor([word_to_index['<PAD>']] * max_fact_length)).view(1, -1))\n",
    "\n",
    "        padded_fact = torch.cat(padded_fact)\n",
    "        padded_facts.append(padded_fact)\n",
    "        fact_masks.append(torch.cat([Variable(ByteTensor(tuple(map(lambda s: s == 0, t.data))), volatile=False) for t in padded_fact]).view(padded_fact.size(0), -1))\n",
    "\n",
    "        if questions[i].size(1) < max_question_length:\n",
    "            padded_questions.append(torch.cat([questions[i], Variable(LongTensor([word_to_index['<PAD>']] * (max_question_length - questions[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            padded_questions.append(questions[i])\n",
    "\n",
    "        if answers[i].size(1) < max_answer_length:\n",
    "            padded_answers.append(torch.cat([answers[i], Variable(LongTensor([word_to_index['<PAD>']] * (max_answer_length - answers[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            padded_answers.append(answers[i])\n",
    "\n",
    "    questions_tensor = torch.cat(padded_questions)\n",
    "    answers_tensor = torch.cat(padded_answers)\n",
    "    question_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s == 0, t.data))), volatile=False) for t in questions_tensor]).view(questions_tensor.size(0), -1)\n",
    "    \n",
    "    return padded_facts, fact_masks, questions_tensor, question_masks, answers_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACmzoIUXwCZF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy\n",
    "\n",
    "def sequence_to_tensor(sequence, word_to_index_map):\n",
    "    index_list = [word_to_index_map.get(word, word_to_index_map[\"<UNK>\"]) for word in sequence]\n",
    "    return Variable(LongTensor(index_list))\n",
    "\n",
    "data_file_path = 'data/en-10k/qa17_positional-reasoning_train.txt'\n",
    "raw_data = open(data_file_path).readlines()\n",
    "processed_data = [line.strip() for line in raw_data]\n",
    "\n",
    "training_examples = []\n",
    "current_facts = []\n",
    "for line in processed_data:\n",
    "    line_index = line.split(' ')[0]\n",
    "    if line_index == '1':\n",
    "        current_facts = []\n",
    "\n",
    "    if '?' in line:\n",
    "        question_and_answer = line.split('\\t')\n",
    "        question = question_and_answer[0].strip().replace('?', '').split(' ')[1:] + ['?']\n",
    "        answer = question_and_answer[1].split() + ['</s>']\n",
    "        copied_facts = deepcopy(current_facts)\n",
    "        training_examples.append([copied_facts, question, answer])\n",
    "    else:\n",
    "        fact = line.replace('.', '').split(' ')[1:] + ['</s>']\n",
    "        current_facts.append(fact)\n",
    "\n",
    "facts, questions, answers = zip(*training_examples)\n",
    "vocabulary = set(flatten(flatten(facts)) + flatten(questions) + flatten(answers))\n",
    "word_to_index_map = {'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vocab_word in vocabulary:\n",
    "    if vocab_word not in word_to_index_map:\n",
    "        word_to_index_map[vocab_word] = len(word_to_index_map)\n",
    "\n",
    "index_to_word_map = {index: word for word, index in word_to_index_map.items()}\n",
    "\n",
    "for sample in training_examples:\n",
    "    for i, fact in enumerate(sample[0]):\n",
    "        sample[0][i] = sequence_to_tensor(fact, word_to_index_map).view(1, -1)\n",
    "    sample[1] = sequence_to_tensor(sample[1], word_to_index_map).view(1, -1)\n",
    "    sample[2] = sequence_to_tensor(sample[2], word_to_index_map).view(1, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zqb_jf4R9OhG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class DMN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(DMN, self).__init__()\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.fact_gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.ques_gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.attn_weights = nn.Sequential(nn.Linear(4*hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 1), nn.Softmax())\n",
    "        \n",
    "        self.epsisodic_grucell = nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.memory_grucell = nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.ans_grucell = nn.GRUCell(2*hidden_size, hidden_size)\n",
    "        \n",
    "        self.ans_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    \n",
    "    def init_hidden(self, inputs):\n",
    "        hidden = Variable(torch.zeros(1, inputs.size(0), self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform_(self.embedding.state_dict()['weight'])\n",
    "\n",
    "        for name, param in self.fact_gru.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal_(param)\n",
    "        for name, param in self.ques_gru.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal_(param)\n",
    "        for name, param in self.attn_weights.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal_(param)\n",
    "        for name, param in self.epsisodic_grucell.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal_(param)\n",
    "        for name, param in self.memory_grucell.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal_(param)\n",
    "        for name, param in self.ans_grucell.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal_(param)\n",
    "\n",
    "        nn.init.xavier_normal_(self.ans_fc.state_dict()['weight'])\n",
    "        self.ans_fc.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, facts, facts_masks, question, question_masks, num_decode, episodes=3, is_training=True):\n",
    "        #input module\n",
    "        concated=[]\n",
    "        for fact, fact_mask in zip(facts, facts_masks):\n",
    "            embedded = self.embedding(fact)\n",
    "            if(is_training):\n",
    "                embedded = self.dropout(embedded)\n",
    "            hidden = self.init_hidden(fact)\n",
    "            output, hidden = self.fact_gru(embedded, hidden)\n",
    "            hidden_real = []\n",
    "            for i, o in enumerate(output):\n",
    "                length = fact_mask[i].data.tolist().count(0)\n",
    "                hidden_real.append(o[length-1])\n",
    "            concated.append(torch.cat(hidden_real).view(fact.size(0), -1).unsqueeze(0)) \n",
    "        encoded_facts = torch.cat(concated)\n",
    "        #question module\n",
    "        hidden=self.init_hidden(question)\n",
    "       \n",
    "        embedded = self.embedding(question)\n",
    "        if(is_training):\n",
    "                embedded = self.dropout(embedded)\n",
    "        output, hidden = self.ques_gru(embedded, hidden)\n",
    "\n",
    "        if is_training == True:\n",
    "            real_question = []\n",
    "            for i, o in enumerate(output): # B,T,D\n",
    "                real_length = question_masks[i].data.tolist().count(0)\n",
    "\n",
    "                real_question.append(o[real_length - 1])\n",
    "\n",
    "            encoded_question = torch.cat(real_question).view(questions.size(0), -1) # B,D\n",
    "        else: # for inference mode\n",
    "            encoded_question = hidden.squeeze(0) # B,D\n",
    "            \n",
    "        #episodic memory module\n",
    "        \n",
    "        memory = encoded_question\n",
    "        T_C = encoded_facts.size(1)\n",
    "        B = encoded_facts.size(0)\n",
    "        for i in range(episodes):\n",
    "            hidden = self.init_hidden(encoded_facts.transpose(0, 1)[0]).squeeze(0) # B,D\n",
    "            for t in range(T_C):\n",
    "               \n",
    "                z = torch.cat([\n",
    "                                    encoded_facts.transpose(0, 1)[t] * encoded_question, # B,D , element-wise product\n",
    "                                    encoded_facts.transpose(0, 1)[t] * memory, # B,D , element-wise product\n",
    "                                    torch.abs(encoded_facts.transpose(0,1)[t] - encoded_question), # B,D\n",
    "                                    torch.abs(encoded_facts.transpose(0,1)[t] - memory) # B,D\n",
    "                                ], 1)\n",
    "                g_t = self.attn_weights(z) # B,1 scalar\n",
    "                hidden = g_t * self.epsisodic_grucell(encoded_facts.transpose(0, 1)[t], hidden) + (1 - g_t) * hidden\n",
    "                \n",
    "            e = hidden\n",
    "            memory = self.memory_grucell(e, memory)\n",
    "        \n",
    "        # Answer Module\n",
    "        answer_hidden = memory\n",
    "        start_decode = Variable(LongTensor([[word_to_index_map['<s>']] * memory.size(0)])).transpose(0, 1)\n",
    "        y_t_1 = self.embedding(start_decode).squeeze(1) # B,D\n",
    "        \n",
    "        decodes = []\n",
    "        for t in range(num_decode):\n",
    "            answer_hidden = self.ans_grucell(torch.cat([y_t_1, encoded_question], 1), answer_hidden)\n",
    "            decodes.append(F.log_softmax(self.ans_fc(answer_hidden),1))\n",
    "        return torch.cat(decodes, 1).view(B * num_decode, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOllKVADVuPD"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 50\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCH = 50\n",
    "NUM_EPISODE = 3\n",
    "EARLY_STOPPING = False\n",
    "EMBEDDING_DIM =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "id": "rSEy0baKjPGF",
    "outputId": "2a8c4f1c-fb61-4409-b951-b238ed076004"
   },
   "outputs": [],
   "source": [
    "model = DMN(len(word_to_index_map), HIDDEN_SIZE, len(word_to_index_map))\n",
    "model.init_weight()\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50] mean_loss : 3.20\n",
      "[0/50] mean_loss : 0.86\n",
      "[1/50] mean_loss : 0.35\n",
      "[1/50] mean_loss : 0.35\n",
      "[2/50] mean_loss : 0.35\n",
      "[2/50] mean_loss : 0.35\n",
      "[3/50] mean_loss : 0.35\n",
      "[3/50] mean_loss : 0.35\n",
      "[4/50] mean_loss : 0.35\n",
      "[4/50] mean_loss : 0.35\n",
      "[5/50] mean_loss : 0.34\n",
      "[5/50] mean_loss : 0.35\n",
      "[6/50] mean_loss : 0.35\n",
      "[6/50] mean_loss : 0.35\n",
      "[7/50] mean_loss : 0.35\n",
      "[7/50] mean_loss : 0.35\n",
      "[8/50] mean_loss : 0.35\n",
      "[8/50] mean_loss : 0.35\n",
      "[9/50] mean_loss : 0.34\n",
      "[9/50] mean_loss : 0.35\n",
      "[10/50] mean_loss : 0.35\n",
      "[10/50] mean_loss : 0.35\n",
      "[11/50] mean_loss : 0.35\n",
      "[11/50] mean_loss : 0.35\n",
      "[12/50] mean_loss : 0.35\n",
      "[12/50] mean_loss : 0.35\n",
      "[13/50] mean_loss : 0.35\n",
      "[13/50] mean_loss : 0.35\n",
      "[14/50] mean_loss : 0.34\n",
      "[14/50] mean_loss : 0.35\n",
      "[15/50] mean_loss : 0.35\n",
      "[15/50] mean_loss : 0.35\n",
      "[16/50] mean_loss : 0.35\n",
      "[16/50] mean_loss : 0.35\n",
      "[17/50] mean_loss : 0.35\n",
      "[17/50] mean_loss : 0.35\n",
      "[18/50] mean_loss : 0.34\n",
      "[18/50] mean_loss : 0.35\n",
      "[19/50] mean_loss : 0.35\n",
      "[19/50] mean_loss : 0.35\n",
      "[20/50] mean_loss : 0.36\n",
      "[20/50] mean_loss : 0.35\n",
      "[21/50] mean_loss : 0.35\n",
      "[21/50] mean_loss : 0.35\n",
      "[22/50] mean_loss : 0.34\n",
      "[22/50] mean_loss : 0.35\n",
      "[23/50] mean_loss : 0.36\n",
      "[23/50] mean_loss : 0.35\n",
      "[24/50] mean_loss : 0.35\n",
      "[24/50] mean_loss : 0.35\n",
      "[25/50] mean_loss : 0.35\n",
      "[25/50] mean_loss : 0.35\n",
      "[26/50] mean_loss : 0.35\n",
      "[26/50] mean_loss : 0.35\n",
      "[27/50] mean_loss : 0.35\n",
      "[27/50] mean_loss : 0.35\n",
      "[28/50] mean_loss : 0.34\n",
      "[28/50] mean_loss : 0.35\n",
      "[29/50] mean_loss : 0.35\n",
      "[29/50] mean_loss : 0.35\n",
      "[30/50] mean_loss : 0.35\n",
      "[30/50] mean_loss : 0.35\n",
      "[31/50] mean_loss : 0.35\n",
      "[31/50] mean_loss : 0.35\n",
      "[32/50] mean_loss : 0.35\n",
      "[32/50] mean_loss : 0.35\n",
      "[33/50] mean_loss : 0.35\n",
      "[33/50] mean_loss : 0.35\n",
      "[34/50] mean_loss : 0.34\n",
      "[34/50] mean_loss : 0.35\n",
      "[35/50] mean_loss : 0.35\n",
      "[35/50] mean_loss : 0.35\n",
      "[36/50] mean_loss : 0.33\n",
      "[36/50] mean_loss : 0.35\n",
      "[37/50] mean_loss : 0.36\n",
      "[37/50] mean_loss : 0.35\n",
      "[38/50] mean_loss : 0.35\n",
      "[38/50] mean_loss : 0.35\n",
      "[39/50] mean_loss : 0.34\n",
      "[39/50] mean_loss : 0.35\n",
      "[40/50] mean_loss : 0.36\n",
      "[40/50] mean_loss : 0.35\n",
      "[41/50] mean_loss : 0.35\n",
      "[41/50] mean_loss : 0.35\n",
      "[42/50] mean_loss : 0.34\n",
      "[42/50] mean_loss : 0.34\n",
      "[43/50] mean_loss : 0.28\n",
      "[43/50] mean_loss : 0.30\n",
      "[44/50] mean_loss : 0.28\n",
      "[44/50] mean_loss : 0.29\n",
      "[45/50] mean_loss : 0.32\n",
      "[45/50] mean_loss : 0.30\n",
      "[46/50] mean_loss : 0.32\n",
      "[46/50] mean_loss : 0.29\n",
      "[47/50] mean_loss : 0.30\n",
      "[47/50] mean_loss : 0.29\n",
      "[48/50] mean_loss : 0.25\n",
      "[48/50] mean_loss : 0.29\n",
      "[49/50] mean_loss : 0.28\n",
      "[49/50] mean_loss : 0.29\n"
     ]
    }
   ],
   "source": [
    "losses1=[]\n",
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    if EARLY_STOPPING: \n",
    "        break\n",
    "        \n",
    "    for i,batch in enumerate(generate_batches(BATCH_SIZE, training_examples)):\n",
    "        facts, fact_masks, questions, question_masks, answers = pad_batch(batch, word_to_index_map)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        pred = model(facts, fact_masks, questions, question_masks, answers.size(1), NUM_EPISODE, True)\n",
    "        loss = loss_function(pred, answers.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "            \n",
    "            if np.mean(losses) < 0.01:\n",
    "                EARLY_STOPPING = True\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "            losses = []\n",
    "    losses1.append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[0.3618998447699206, 0.3515688691820417, 0.34927765652537346, 0.34830011054873466, 0.34925996884703636, 0.34776782404099194, 0.3480599213923727, 0.34823588973709513, 0.3476379561637129, 0.34734330805284636, 0.34749370547277586, 0.3476337944822652, 0.34747676870652605, 0.3475588650575706, 0.3473850642996175, 0.347673128758158, 0.34760779195598196, 0.3470092373234885, 0.34738509729504585, 0.3470698211874281, 0.3471668948020254, 0.34717084041663576, 0.34761867246457506, 0.34732159812535557, 0.3467982267694814, 0.34719036679182735, 0.3475447419498648, 0.34732286791716305, 0.3473794875400407, 0.3477414564362594, 0.34672645958406584, 0.3464804786656584, 0.34758840607745306, 0.3459325741444315, 0.34581958981496946, 0.34779562641467365, 0.3473358835492815, 0.3464526906609535, 0.34702711392726215, 0.34675620443054606, 0.34903718744005474, 0.3462210127285549, 0.30948749982884954, 0.2984617693083627, 0.2968173152101891, 0.29140418900975157, 0.2935704852321318, 0.2922914257006986, 0.2903997435101441, 0.2842198655541454]\n"
     ]
    }
   ],
   "source": [
    "print(len(losses1))\n",
    "print(losses1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 62.6%\n",
      "Facts:\n",
      " The pink rectangle is above the blue square </s>\n",
      "The yellow square is above the pink rectangle </s>\n",
      "\n",
      "Question: Is the yellow square above the blue square ?\n",
      "\n",
      "Answer: yes </s>\n",
      "Prediction: yes </s>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy\n",
    "\n",
    "def pad_to_max_length(facts, token_to_index):\n",
    "    max_length = max(fact.size(1) for fact in facts)\n",
    "    padded_facts = []\n",
    "    for fact in facts:\n",
    "        if fact.size(1) < max_length:\n",
    "            padding = Variable(LongTensor([token_to_index['<PAD>']] * (max_length - fact.size(1))))\n",
    "            padded_fact = torch.cat([fact, padding.view(1, -1)], 1)\n",
    "            padded_facts.append(padded_fact)\n",
    "        else:\n",
    "            padded_facts.append(fact)\n",
    "        \n",
    "    concatenated_facts = torch.cat(padded_facts)\n",
    "    fact_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s == 0, tensor.data))), volatile=False) for tensor in concatenated_facts]).view(concatenated_facts.size(0), -1)\n",
    "    return concatenated_facts, fact_masks\n",
    "\n",
    "def prepare_data(file_path, word_to_index_map):\n",
    "    raw_data = open(file_path).readlines()\n",
    "    processed_data = [line.strip() for line in raw_data]\n",
    "    data = []\n",
    "    facts = []\n",
    "\n",
    "    for line in processed_data:\n",
    "        line_index = line.split(' ')[0]\n",
    "        if line_index == '1':\n",
    "            facts = []\n",
    "\n",
    "        if '?' in line:\n",
    "            parts = line.split('\\t')\n",
    "            question = parts[0].strip().replace('?', '').split(' ')[1:] + ['?']\n",
    "            answer = parts[1].split() + ['</s>']\n",
    "            copied_facts = deepcopy(facts)\n",
    "            data.append([copied_facts, question, answer])\n",
    "        else:\n",
    "            fact = line.replace('.', '').split(' ')[1:] + ['</s>']\n",
    "            facts.append(fact)\n",
    "\n",
    "    for item in data:\n",
    "        for i, fact in enumerate(item[0]):\n",
    "            item[0][i] = sequence_to_tensor(fact, word_to_index_map).view(1, -1)\n",
    "        item[1] = sequence_to_tensor(item[1], word_to_index_map).view(1, -1)\n",
    "        item[2] = sequence_to_tensor(item[2], word_to_index_map).view(1, -1)\n",
    "\n",
    "    return data\n",
    "\n",
    "def evaluate_model(test_data, model, token_to_index, num_episodes, index_to_word_map):\n",
    "    accuracy = 0\n",
    "    for test_case in test_data:\n",
    "        facts, facts_mask = pad_to_max_length(test_case[0], token_to_index)\n",
    "        question = test_case[1]\n",
    "        question_mask = Variable(ByteTensor([0] * question.size(1)), requires_grad=False).unsqueeze(0)\n",
    "        answer = test_case[2].squeeze(0)\n",
    "\n",
    "        model.zero_grad()\n",
    "        prediction = model([facts], [facts_mask], question, question_mask, answer.size(0), num_episodes, False)\n",
    "        if prediction.max(1)[1].tolist() == answer.tolist():\n",
    "            accuracy += 1\n",
    "\n",
    "    accuracy_percentage = accuracy / len(test_data) * 100\n",
    "    return accuracy_percentage\n",
    "\n",
    "def display_example(test_data, model, token_to_index, num_episodes, index_to_word_map):\n",
    "    test_case = random.choice(test_data)\n",
    "    facts, facts_mask = pad_to_max_length(test_case[0], token_to_index)\n",
    "    question = test_case[1]\n",
    "    question_mask = Variable(ByteTensor([0] * question.size(1)), requires_grad=False).unsqueeze(0)\n",
    "    answer = test_case[2].squeeze(0)\n",
    "\n",
    "    model.zero_grad()\n",
    "    prediction = model([facts], [facts_mask], question, question_mask, answer.size(0), num_episodes, False)\n",
    "\n",
    "    facts_text = '\\n'.join([' '.join([index_to_word_map[ix] for ix in fact_row]) for fact_row in facts.tolist()])\n",
    "    question_text = ' '.join([index_to_word_map[ix] for ix in question[0].tolist()])\n",
    "    answer_text = ' '.join([index_to_word_map[ix] for ix in answer.tolist()])\n",
    "    prediction_text = ' '.join([index_to_word_map[ix] for ix in prediction.max(1)[1].tolist()])\n",
    "\n",
    "    print(\"Facts:\\n\", facts_text)\n",
    "    print(\"\\nQuestion:\", question_text)\n",
    "    print(\"\\nAnswer:\", answer_text)\n",
    "    print(\"Prediction:\", prediction_text)\n",
    "\n",
    "# Assuming other parts of the code, like model definition, are already provided\n",
    "\n",
    "# Load and prepare data\n",
    "test_file_path = 'data/en-10k/qa17_positional-reasoning_test.txt'\n",
    "test_data = prepare_data(test_file_path, word_to_index_map)\n",
    "\n",
    "# Evaluate model\n",
    "model_accuracy = evaluate_model(test_data, model, word_to_index_map, NUM_EPISODE, index_to_word_map)\n",
    "print(f\"Model Accuracy: {model_accuracy}%\")\n",
    "\n",
    "# Display an example\n",
    "display_example(test_data, model, word_to_index_map, NUM_EPISODE, index_to_word_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "cL8hWml6eW-H",
    "outputId": "453ac8cc-dc89-4002-8dcb-5f6eea1b8c75"
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'DMN4.pkl')\n",
    "# Uncomment to load the existing model\n",
    "# model = torch.load('DMN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOlv8DHFGbv5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the loss vs epochs\n",
    "epochs = range(1, len(losses1) + 1)\n",
    "plt.plot(epochs, losses1, 'bo-', label='Training loss')\n",
    "plt.title('Training Loss vs Epochs for Task 17')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Dynamic Memory Question Answering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
